from agent.agent_class import *
import argparse
import random

class RAGAgentTTS(AgentTTS):

    def __init__(self, name, desc, budget, model_choices, pattern, sub_tasks, Np_list, Nd_list, llm, example):
        super().__init__(name, desc, budget, model_choices, pattern, sub_tasks, Np_list, Nd_list, llm, example)

    def _gen_new_candidates(self, batch_size):
        system_prompt = "You are an expert in parameter optimization."
        experience_list = self.archive.experience[:1] + self.archive.experience[-2:]
        experience = "\n".join(experience_list)
        history_str = "\n".join(
            [
                f"Parameters: {h['params']}, Score: {h['score']}, Used budget: {h['cost']:.2f}, total budget: {self.budget}"
                for h in self.archive.history[:]]
        )
        maximal_model_str = ""
        for sub_task, v in self.M_dict.items():
            maximal_model_str += sub_task + ": " + str(v) + "B\n"
        prompt = f"""I am tuning test-time parameters for a complex task that can be broken down into multiple simple subtasks. The parameters include the model size and the number of samples used for each subtask. Generally, increasing the number of samples improves performance. The goal is to find the best parameter settings that maximize performance within a fixed budget.
        
        I will provide the task name, task description, a list of subtasks and the model choices available for each, the total budget, available samples, the budget calculation formula, evaluation metrics (including the main metric to optimize), search history (previous configurations tried), and insights from previous experiments.

        The budget for a subtask is calculated using 
        The budget for a subtask is calculated using this code:
        ```python
        def compute_budget(num_samples, model_size, prompt_length, generation_length, M_small=3e9, Np2=128, Nd2=64):
            # Compute the FLOPs-based budget of generating num_samples samples on a large model,
            # normalized to the FLOPs of a small model's unit budget.
            # 
            # Parameters:
            # - num_samples: number of samples generated by the large model
            # - M_small: parameter count of the small model (e.g., 3B)
            # - model_size: parameter count of the large model (e.g., 72B, 70B, 32B, 8B, 7B, 3B)
            # - Np2: prompt length for small model (unit config)
            # - Nd2: decode length for small model (unit config)
            # - prompt_length: prompt length for the new config
            # - generation_length: decode length for the new config
            # 
            # Returns:
            # - budget: normalized compute budget
            alpha = model_size / M_small
            beta1 = prompt_length / generation_length
            beta2 = Np2 / Nd2  # normalize to same decode length as unit config
            beta3 = prompt_length / Np2
        
            budget = beta2 * ((alpha * beta3 / beta1) * (beta1 + num_samples) - 1)
            return budget
        ```
        and this formula:
        Budget(model_size, number_of_samples, prompt_length, generation_length | subtask) = number_of_samples * (1/2 * generation_length/prompt_length) + (model_size / 3e9 * prompt_length/64 -1)/2. 
        Here, model_size is the selected model size for the subtask, such as 8e9=8B, number_of_samples is how many samples are used, and prompt_length is the average prompt length and generation_length is the generation length for that subtask.
        The total budget is the sum of the budgets across all subtasks.
        
        Using the information I provide, please generate {batch_size} new candidate configurations that follow the insights and stay within the total budget. The output must be in strict JSON format, using the structure shown below:
        
        {json.dumps(self.example, indent=2)}
        
        Inputs: 
        ###Task name: {self.task_name}
        ###Task description: {self.task_desc}
        ###Subtasks, model choices, and prompt_length, generation_length: {model_space}
        ###Available samples: [1, 100]
        ###Total budget: {self.budget}
        ###Budget equation per subtask: as described above
        ###Metrics: {self.archive.environment.metrics}
        ###Main metric: {self.archive.environment.main_metric}
        ###Search history: {history_str}
        ###Insights: {experience}
        
        Your response should return exactly {batch_size} new candidates, in strict JSON format only.
        
        ###Response: 
        """
        response = self.llm.generate(prompt, system_prompt)
        response_dict = self.extract_json(response)
        print('Log, generated new candidates by LLM:', response_dict)
        return response_dict['candidates']

    def gen_new_candidates(self, batch_size):
        system_prompt = "You are an expert in parameter optimization."
        recent_experience = self.archive.experience[-1]
        prompt = f"""I am tuning test-time parameters for a complex task that can be broken down into multiple simple subtasks. The parameters include the model size and the number of samples used for each subtask. Generally, increasing the number of samples improves performance. The goal is to find the best parameter settings that maximize performance within a fixed budget.

        I will provide the most recent search experience. Based on that, please help me decide the next action by choosing one of the following options:
        (A) Generate new candidates by yourself.
        (B) Generate new candidates using crossover, mutation, or random variation.
        
        ###Experience: {recent_experience}
        
        Only return the most appropriate option (A) or (B) based on the experience, and no additional text.
        
        ###Response: 
        """
        # response = self.llm.generate(prompt, system_prompt)
        # print('Log, new candidates generation mode:', response)
        response = 'A'

        if 'A' in response:
            # generate by LLM
            print("\n\n### generating new candidates using LLM Agent.")
            new_candidates = self._gen_new_candidates(batch_size)
        elif 'B' in response:
            # generate by crossover, mutation, and random
            print("\n\n### generating new candidates using crossover, mutation or random variation.")
            history = self.archive.history
            mutation_rate = 0.5
            crossover_rate = 0.8
            new_candidates1 = self.evolve(history, batch_size//2, crossover_rate, mutation_rate)
            new_candidates2 = self.random(batch_size - batch_size//2)
            new_candidates = new_candidates1 + new_candidates2

        validated = []
        for candidate in new_candidates:
            if self.check_budget(candidate):
                validated.append(candidate)
        return validated[:batch_size]

    def random(self, batch_size):
        candidates = []
        for subtask, model_choice in zip(self.sub_tasks, self.model_choices):
            if len(self.archive.history) > 0:
                individual = self.archive.history[0]['params'].copy()
            else:
                individual = {}
                for subtask, model_choice in zip(self.sub_tasks, self.model_choices):
                    individual[subtask] = {}
                    individual[subtask]['model'] = model_choice[0]
                    individual[subtask]['samples'] = 1
            individual[subtask]["model"] = random.choice(model_choice)
            maximal_model_size = self.M_dict[subtask]
            model_size = self.get_model_size(self.pattern, individual[subtask]["model"])
            unit_samples = 4 * (maximal_model_size / model_size) - 3
            individual[subtask]["samples"] = random.choice([1] + list(range(10, int(unit_samples), 5)))
            if self.check_budget(individual):
                candidates.append(individual)
            if len(candidates) >= batch_size:
                break
        return candidates


    def evolve(self, evaluated_population, population_size, crossover_rate, mutation_rate):
        new_population = []

        elites = sorted(evaluated_population, key=lambda x: x['score'][self.archive.environment.main_metric], reverse=True)[:population_size]  # elite size
        new_population.extend([e['params'] for e in elites])

        while len(new_population) < population_size*2:
            parent1 = self._rank_selection(evaluated_population)
            parent2 = self._rank_selection(evaluated_population)

            if random.random() < crossover_rate:
                child = self._crossover(parent1, parent2)
            else:
                child = parent1.copy()

            child = self._mutate(child, mutation_rate)

            if self.calculate_budget(child) <= self.budget:
                new_population.append(child)

        population = new_population[-population_size:]
        return population

    def _rank_selection(self, evaluated_population):
        sorted_pop = sorted(evaluated_population, key=lambda x: x['score'][self.archive.environment.main_metric], reverse=True)
        ranks = np.arange(1, len(sorted_pop) + 1)
        probs = ranks / ranks.sum()
        return sorted_pop[np.random.choice(len(sorted_pop), p=probs)]['params']

    def _crossover(self, parent1, parent2):
        child = {}
        subtasks = self.sub_tasks
        crossover_point = random.randint(1, len(subtasks) - 1)

        for i, subtask in enumerate(subtasks):
            if i < crossover_point:
                child[subtask] = parent1[subtask].copy()
            else:
                child[subtask] = parent2[subtask].copy()
        return child

    def _mutate(self, individual, mutation_rate):
        mutated = individual.copy()
        for subtask, model_choice in zip(self.sub_tasks, self.model_choices):
            if random.random() < mutation_rate:  # mutation rate
                mutated[subtask]["model"] = random.choice(model_choice)
            if random.random() < mutation_rate:
                current = mutated[subtask]["samples"]
                mutated[subtask]["samples"] = max(1, current + random.choice([-10, 0, 10]))
        return mutated



    def gen_new_experience(self, recent_added_samples:int, init=False):
        if init:
            history_str = "\n".join(
                [f"Parameters: {h['params']}, Score: {h['score']}, Used budget: {h['cost']:.2f}, total budget: {self.budget}" for h in self.archive.history[-recent_added_samples:]]
            )
            system_prompt = "You are an expert in parameter optimization."
            prompt = f"""I am tuning test-time parameters for a complex task that can be broken down into multiple simple subtasks. The parameters include the model size and the number of samples used for each subtask. Generally, increasing the number of samples improves performance. The goal is to find the best parameter settings that maximize performance within a fixed budget.

            I will provide the task name, description, budget, and initial search history. Please help me summarize insights from the search history in the following aspect:
            
            1. The search history includes initial trials that use different model sizes under the same unit budget. In these trials, smaller models are allowed more samples, while larger models have fewer samples due to their higher cost. Please help me identify which model size we should explore further in each subtask. If the large model performs significantly better than the small model, we will choose the large one. Otherwise, if its performance is similar or only slightly better, we will prefer the smaller model.
            
            ###Task name: {self.task_name}  
            ###Task description: {self.task_desc}  
            ###Subtasks: {self.sub_tasks}  
            ###Budget: {self.budget}  
            ###Metrics: {self.archive.environment.metrics}, Main metric: {self.archive.environment.main_metric}  
            
            ###Search history: {history_str}  
            
            Please generate your response in most concise wording.
            
            ###Response:
            """
        else:
            if not no_insight2:
                insight2 = "As the number of samples increases, performance tends to improve and then level off. Please identify the search direction for finding the optimal number of samples for each subtask, especially the point after which more samples no longer improve results."
            else:
                insight2 = "None"
            if not no_insight3:
                insight3 = "The total compute budget is shared across all subtasks and they are interdependent. If we could not assign the optimal model and sample count to every subtask, consider the following strategy. Prioritize each subtask and try different combinations of the budget allocation in other tasks and Balance the allocations among subtasks. "
                
            else:
                insight3 = "None"
            old_history_str = "\n".join(
                [f"Parameters: {h['params']}, Score: {h['score']}, Used budget: {h['cost']:.2f}, total budget: {self.budget}" for h in self.archive.history[:-recent_added_samples]]
            )
            recent_history_str = "\n".join([f"Parameters: {h['params']}, Score: {h['score']}, Used budget: {h['cost']:.2f}, total budget: {self.budget}" for h in self.archive.history[-recent_added_samples:]])
            system_prompt = "You are an expert in parameter optimization."
            prompt = f"""I am tuning test-time parameters for a complex task that can be broken down into multiple simple subtasks. The parameters include the model size and the number of samples used for each subtask. Generally, increasing the number of samples improves performance. The goal is to find the best parameter settings that maximize performance within a fixed budget.

            I will provide the task name, description, budget, and both old and recent search history.
            
            Please help me summarize insights from the recent search history in the following three aspects:
            
            1. {insight2}
            
            2. {insight3}
            
            3. If you observe that the performance variance across the search history is small, please suggest that future searches focus more on exploration strategies such as crossover, mutation, and random search, rather than strictly following the existing search patterns.
            
            ###Task name: {self.task_name}  
            ###Task description: {self.task_desc}  
            ###Subtasks: {self.sub_tasks}  
            ###Budget: {self.budget}  
            ###Metrics: {self.archive.environment.metrics}, Main metric: {self.archive.environment.main_metric}  
            
            ###Old search history: {old_history_str}  
            ###Recent search history: {recent_history_str}  
            
            Please generate your response in most concise wording, item by item.
            
            ###Response:
            """
        print("\nLog, prompt for experience:", prompt)
        experience = self.llm.generate(prompt, system_prompt)
        print("\nLog, experience:", experience)

        return experience

    def convert_budget_to_num_samples(self, budget, current_model_size, Np, Nd):
        M_large = current_model_size
        M_small = 3e9
        alpha = M_large / M_small
        beta1 = Np / Nd
        beta2 = 128 / 64
        beta3 = Np / 128
        S = beta1 * (((budget / beta2) + 1) / (alpha * beta3) - 1)
        return S

    def convert_num_samples_to_budget(self, S, M_large, Np1, Nd1, M_small=3e9, Np2=128, Nd2=64):

        alpha = M_large / M_small
        beta1 = Np1 / Nd1
        beta2 = Np2 / Nd2  # normalize to same decode length as unit config
        beta3 = Np1 / Np2

        budget = beta2 * ((alpha * beta3 / beta1) * (beta1 + S) - 1)

        return budget

    def init_candidates(self):
        candidates = []
        budget_for_subtasks = self.budget / len(self.sub_tasks)
        if self.task_name in ["2wikihopqa","2wikihopqa_75","2wikihopqa_100", "hotpotqa"]:
            budget_for_subtasks = [self.budget*0.85, self.budget*0.15]
        elif self.task_name in ["cwq", "webqsp"]:
            budget_for_subtasks = [self.budget*0.78, self.budget*0.22]
        elif self.task_name in ["taskbench_dailylifeapis"]:
            budget_for_subtasks = [self.budget*0.2, self.budget*0.24, self.budget*0.56]
        elif self.task_name in ["chatdev"]:
            budget_for_subtasks = [self.budget*0.42, self.budget*0.32, self.budget*0.26]

        if not no_insight1:
            for sub_task, budget, model_choice, Np, Nd in zip(self.sub_tasks, budget_for_subtasks, self.model_choices, Np_list, Nd_list):
                max_model_size = self.M_dict[sub_task]
                # budget =
                lowest_budget_for_others = 0
                for sub_task2, model_choice2, Np2, Nd2 in zip(self.sub_tasks, self.model_choices, Np_list, Nd_list):
                    if sub_task2 != sub_task:
                        lowest_model_choice = model_choice2[0]
                        model_size = self.get_model_size(self.pattern, lowest_model_choice)
                        lowest_budget = self.convert_num_samples_to_budget(1, model_size, Np2, Nd2)
                        lowest_budget_for_others += lowest_budget
                highest_budget_for_others = 0
                for sub_task2, model_choice2, Np2, Nd2 in zip(self.sub_tasks, self.model_choices, Np_list, Nd_list):
                    if sub_task2 != sub_task:
                        highest_model_choice = model_choice2[-1]
                        model_size = self.get_model_size(self.pattern, highest_model_choice)
                        highest_budget = self.convert_num_samples_to_budget(1, model_size, Np2, Nd2)
                        highest_budget_for_others += highest_budget
                budget_max_available = self.budget - lowest_budget_for_others
                budget_for_current_max_model_sampling1 = self.convert_num_samples_to_budget(1, max_model_size, Np, Nd)
                if budget_for_current_max_model_sampling1 < budget_max_available:
                    budget = budget_for_current_max_model_sampling1
                else:
                    budget = budget_max_available
                for model in model_choice:
                    candidate1 = {}
                    current_model_size = self.get_model_size(self.pattern, model)  # model_choice[0] is the minimal
                    # denominator = 4 * (max_model_size / current_model_size) - 3
                    candidate1[sub_task] = {}
                    candidate1[sub_task]['model'] = model
                    S = self.convert_budget_to_num_samples(budget, current_model_size, Np, Nd)
                    if S < 1:
                        continue
                    S = 60 if S > 60 else S
                    # budget = S / (2 * beta_1) + (alpha * beta_3 - 1) / 2
                    candidate1[sub_task]['budget'] = budget
                    candidate1[sub_task]['samples'] = int(S)
                    for sub_task2, budget2, model_choice2, Np2, Nd2 in zip(self.sub_tasks, budget_for_subtasks, self.model_choices, Np_list, Nd_list):
                        budget_remaining = self.budget - budget
                        budget2 = budget_remaining
                        if sub_task2 != sub_task:
                            candidate1[sub_task2] = {}
                            if budget2 > highest_budget_for_others:
                                current_model_size2 = self.get_model_size(self.pattern, model_choice2[-1])
                                budget_unit = self.convert_num_samples_to_budget(1, current_model_size2, Np2, Nd2)
                                print("log, budget_unit:", budget_unit)
                                candidate1[sub_task2]['model'] = model_choice2[-1]
                                candidate1[sub_task2]['samples'] = 1
                                candidate1[sub_task2]['budget'] = budget_unit
                            else:
                                current_model_size2 = self.get_model_size(self.pattern, model_choice2[0])
                                budget_unit = self.convert_num_samples_to_budget(1, current_model_size2, Np2, Nd2)
                                print("log, budget_unit:", budget_unit)
                                candidate1[sub_task2]['model'] = model_choice2[0]
                                candidate1[sub_task2]['samples'] = 1
                                candidate1[sub_task2]['budget'] = budget_unit

                    candidates.append(candidate1)
        else:
            candidates = self.random(1)
        return candidates



if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--task_name', type=str, default="2wikihopqa")
    parser.add_argument('--budget', type=int, default=1000,)
    parser.add_argument('--iterations', type=int, default=5,)
    parser.add_argument('--batch_size', type=int, default=10,)
    parser.add_argument('--no_insight1', action='store_true')
    parser.add_argument('--no_insight2', action='store_true')
    parser.add_argument('--no_insight3', action='store_true')

    args = parser.parse_args()
    print(args)

    unit_budgets = {
        "2wikihopqa": {"Retrieval": 814, "Question Answering": 115},
        "2wikihopqa_75": {"Retrieval": 814, "Question Answering": 115},
        "2wikihopqa_100": {"Retrieval": 814, "Question Answering": 115},
        "hotpotqa": {},
        "cwq": {},
        "webqsp": {},
        "taskbench_dailylifeapis": {"Task Decomposition": 395, "Tool Selection": 465, "Parameter Prediction": 1118},
        "chatdev": {"Coding": 745, "Static Testing": 558, "Dynamic Testing": 465},
    }
    # for ablation study
    no_insight1 = args.no_insight1
    no_insight2 = args.no_insight2
    no_insight3 = args.no_insight3
    print("\nNo insights (1, 2, 3):", no_insight1, no_insight2, no_insight3)
    name = args.task_name
    budget = args.budget
    if "2wikihopqa" in name:
        desc = "This task Retreval-then-generate has two subtasks. The first subtask is retrieval: given a question and a set of 100 text chunks, the model needs to select the most relevant chunks that can help answer the question. The second subtask is question answering: based on the retrieved chunks, the model should generate a response that answers the question accurately."
        model_choices = [["Qwen/Qwen2.5-7B-Instruct-AWQ", "Qwen/Qwen2.5-32B-Instruct-AWQ", "Qwen/Qwen2.5-72B-Instruct-AWQ"], ["meta-llama/Llama-3.2-3B-Instruct", "meta-llama/Llama-3.1-8B-Instruct", "meta-llama/Llama-3.1-70B-Instruct"]]
        pattern = r"-(\d+)B-"
        sub_tasks = ["Retrieval", "Question Answering"]
        Np_list = [2048, 256]
        Nd_list = [128, 64]
        model_space = {
            sub_task: {'model_choice': model_choice, "prompt_length": Np, "generation_length": Nd} for sub_task, model_choice, Np, Nd in zip(sub_tasks, model_choices, Np_list, Nd_list)
        }
    elif name == "hotpotqa":
        desc = "This task Retreval-then-generate has two subtasks. The first subtask is retrieval: given a question and a set of 100 text chunks, the model needs to select the most relevant chunks that can help answer the question. The second subtask is question answering: based on the retrieved chunks, the model should generate a response that answers the question accurately."
        model_choices = [["Qwen/Qwen2.5-7B-Instruct-AWQ", "Qwen/Qwen2.5-32B-Instruct-AWQ", "Qwen/Qwen2.5-72B-Instruct-AWQ"], ["meta-llama/Llama-3.2-3B-Instruct", "meta-llama/Llama-3.1-8B-Instruct", "meta-llama/Llama-3.1-70B-Instruct"]]
        pattern = r"-(\d+)B-"
        sub_tasks = ["Retrieval", "Question Answering"]
        Np_list = [2048, 256]
        Nd_list = [128, 64]
        model_space = {
            sub_task: {'model_choice': model_choice, "prompt_length": Np, "generation_length": Nd} for sub_task, model_choice, Np, Nd in zip(sub_tasks, model_choices, Np_list, Nd_list)
        }
    elif name in ["cwq", "webqsp"]:
        desc = "This task Retreval-then-generate has two subtasks. The first subtask is retrieval: given a question and a set of 100 knowledge triplets, the model needs to select the most relevant triplets that can help answer the question. The second subtask is question answering: based on the retrieved knowledge triplets, the model should generate a response that answers the question accurately."
        model_choices = [["Qwen/Qwen2.5-7B-Instruct-AWQ", "Qwen/Qwen2.5-72B-Instruct-AWQ"], ["meta-llama/Llama-3.1-8B-Instruct", "meta-llama/Llama-3.1-70B-Instruct"]]
        pattern = r"-(\d+)B-"
        sub_tasks = ["Retrieval", "Question Answering"]
        Np_list = [1024, 256]
        Nd_list = [64, 64]
        model_space = {
            sub_task: {'model_choice': model_choice, "prompt_length": Np, "generation_length": Nd} for sub_task, model_choice, Np, Nd in zip(sub_tasks, model_choices, Np_list, Nd_list)
        }
    elif name in ["taskbench_dailylifeapis"]:
        desc = "This task Taskbench has three subtasks: task decomposition, tool selection, and parameter prediction. The first subtask is task decomposition: upon receiving a user request, the large language model performs task decomposition, generating a sequence of task steps. The second subtask is tool selection: for each task step, the model should select appropriate tools. The third subtask is parameter prediction: for each selected tool, the model should predict its parameters."
        model_choices = [["meta-llama/Llama-3.1-8B-Instruct", "meta-llama/Llama-3.1-70B-Instruct"], ["meta-llama/Llama-3.1-8B-Instruct", "meta-llama/Llama-3.1-70B-Instruct"], ["meta-llama/Llama-3.1-8B-Instruct", "meta-llama/Llama-3.1-70B-Instruct"]]
        pattern = r"-(\d+)B-"
        sub_tasks = ["Task Decomposition", "Tool Selection", "Parameter Prediction"]
        Np_list = [1024, 1024, 1024]
        Nd_list = [64, 256, 2048]
        model_space = {
            sub_task: {'model_choice': model_choice, "prompt_length": Np, "generation_length": Nd} for sub_task, model_choice, Np, Nd in zip(sub_tasks, model_choices, Np_list, Nd_list)
        }
    elif name in ["chatdev"]:
        desc = "This task Chatdev is an automated software development task. It has three subtasks: coding, static testing, and dynamic testing."
        model_choices = [["llama3b", "llama70b"],["llama3b", "llama70b"],["llama3b", "llama70b"],]
        pattern = r"-(\d+)B-"
        sub_tasks = ["Coding", "Static Testing", "Dynamic Testing"]
        Np_list = [1024, 1024, 1024]
        Nd_list = [1024, 512, 256]
        model_space = {
            sub_task: {'model_choice': model_choice, "prompt_length": Np, "generation_length": Nd} for sub_task, model_choice, Np, Nd in zip(sub_tasks, model_choices, Np_list, Nd_list)
        }


    # init an agent object
    llm = ChatGPT()
    example = {}
    for sub_task, model_choice in zip(sub_tasks, model_choices):
        example[sub_task] = {}
        example[sub_task]['model'] = model_choice[-1]
        example[sub_task]['samples'] = 1
        example[sub_task]['budget'] = unit_budgets[name][sub_task]
    example = {"candidates": [
        example,
    ]}
    agent = RAGAgentTTS(name, desc, budget, model_choices, pattern, sub_tasks, Np_list, Nd_list, llm, example)
    # init a task object
    hub = Task(name, desc, budget, model_choices, pattern, sub_tasks, agent)
    results, trace = hub.get_tts_strategy(iterations=args.iterations, batch_size=args.batch_size)
    print("Trace:", trace)

