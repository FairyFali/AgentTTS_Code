from agent.agent_class import *
import argparse
import random

class RAGAgentTTS(AgentTTS):

    def __init__(self, name, desc, budget, model_choices, pattern, sub_tasks, Np_list, Nd_list, llm, example):
        super().__init__(name, desc, budget, model_choices, pattern, sub_tasks, Np_list, Nd_list, llm, example)

    def _gen_new_candidates(self, batch_size):
        system_prompt = "You are an expert in parameter optimization."
        experience_list = self.archive.experience[:1] + self.archive.experience[-2:]
        experience = "\n".join(experience_list)
        history_str = "\n".join(
            [
                f"Parameters: {h['params']}, Score: {h['score']}, Used budget: {h['cost']:.2f}, total budget: {self.budget}"
                for h in self.archive.history[:]]
        )
        maximal_model_str = ""
        for sub_task, v in self.M_dict.items():
            maximal_model_str += sub_task + ": " + str(v) + "B\n"
        prompt = f"""I am tuning test-time parameters for a complex task that can be broken down into multiple simple subtasks. The parameters include the model size and the number of samples used for each subtask. Generally, increasing the number of samples improves performance. The goal is to find the best parameter settings that maximize performance within a fixed budget.
        
        I will provide the task name, task description, a list of subtasks and the model choices available for each, the total budget, available samples, the budget calculation formula, evaluation metrics (including the main metric to optimize), search history (previous configurations tried), and insights from previous experiments.

        The budget for a subtask is calculated using 
        The budget for a subtask is calculated using this code:
        ```python
        def compute_budget(num_samples, model_size, prompt_length, generation_length, M_small=3e9, Np2=128, Nd2=64):
            # Compute the FLOPs-based budget of generating num_samples samples on a large model,
            # normalized to the FLOPs of a small model's unit budget.
            # 
            # Parameters:
            # - num_samples: number of samples generated by the large model
            # - M_small: parameter count of the small model (e.g., 3B)
            # - model_size: parameter count of the large model (e.g., 72B, 70B, 32B, 8B, 7B, 3B)
            # - Np2: prompt length for small model (unit config)
            # - Nd2: decode length for small model (unit config)
            # - prompt_length: prompt length for the new config
            # - generation_length: decode length for the new config
            # 
            # Returns:
            # - budget: normalized compute budget
            alpha = model_size / M_small
            beta1 = prompt_length / generation_length
            beta2 = Np2 / Nd2  # normalize to same decode length as unit config
            beta3 = prompt_length / Np2
        
            budget = beta2 * ((alpha * beta3 / beta1) * (beta1 + num_samples) - 1)
            return budget
        ```
        and this formula:
        Budget(model_size, number_of_samples, prompt_length, generation_length | subtask) = number_of_samples * (1/2 * generation_length/prompt_length) + (model_size / 3e9 * prompt_length/64 -1)/2. 
        Here, model_size is the selected model size for the subtask, such as 8e9=8B, number_of_samples is how many samples are used, and prompt_length is the average prompt length and generation_length is the generation length for that subtask.
        The total budget is the sum of the budgets across all subtasks.
        
        Using the information I provide, please generate {batch_size} new candidate configurations that follow the insights and stay within the total budget. The output must be in strict JSON format, using the structure shown below:
        
        {json.dumps(self.example, indent=2)}
        
        Inputs: 
        ###Task name: {self.task_name}
        ###Task description: {self.task_desc}
        ###Subtasks, model choices, and prompt_length, generation_length: {model_space}
        ###Available samples: [1, 100]
        ###Total budget: {self.budget}
        ###Budget equation per subtask: as described above
        ###Metrics: {self.archive.environment.metrics}
        ###Main metric: {self.archive.environment.main_metric}
        ###Search history: {history_str}
        ###Insights: {experience}
        
        Your response should return exactly {batch_size} new candidates, in strict JSON format only.
        
        ###Response: 
        """
        response = self.llm.generate(prompt, system_prompt)
        response_dict = self.extract_json(response)
        print('Log, generated new candidates by LLM:', response_dict)
        return response_dict['candidates']

    def gen_new_candidates(self, batch_size):
        system_prompt = "You are an expert in parameter optimization."
        recent_experience = self.archive.experience[-1]
        prompt = f"""I am tuning test-time parameters for a complex task that can be broken down into multiple simple subtasks. The parameters include the model size and the number of samples used for each subtask. Generally, increasing the number of samples improves performance. The goal is to find the best parameter settings that maximize performance within a fixed budget.

        I will provide the most recent search experience. Based on that, please help me decide the next action by choosing one of the following options:
        (A) Generate new candidates by yourself.
        (B) Generate new candidates using crossover, mutation, or random variation.
        
        ###Experience: {recent_experience}
        
        Only return the most appropriate option (A) or (B) based on the experience, and no additional text.
        
        ###Response: 
        """
        # response = self.llm.generate(prompt, system_prompt)
        # print('Log, new candidates generation mode:', response)
        response = 'A'

        if 'A' in response:
            # generate by LLM
            print("\n\n### generating new candidates using LLM Agent.")
            new_candidates = self._gen_new_candidates(batch_size)
        elif 'B' in response:
            # generate by crossover, mutation, and random
            print("\n\n### generating new candidates using crossover, mutation or random variation.")
            history = self.archive.history
            mutation_rate = 0.5
            crossover_rate = 0.8
            new_candidates1 = self.evolve(history, batch_size//2, crossover_rate, mutation_rate)
            new_candidates2 = self.random(batch_size - batch_size//2)
            new_candidates = new_candidates1 + new_candidates2

        validated = []
        for candidate in new_candidates:
            if self.check_budget(candidate):
                validated.append(candidate)
        return validated[:batch_size]

    def random(self, batch_size):
        candidates = []
        for subtask, model_choice in zip(self.sub_tasks, self.model_choices):
            if len(self.archive.history) > 0:
                individual = self.archive.history[0]['params'].copy()
            else:
                individual = {}
                for subtask, model_choice in zip(self.sub_tasks, self.model_choices):
                    individual[subtask] = {}
                    individual[subtask]['model'] = model_choice[0]
                    individual[subtask]['samples'] = 1
            individual[subtask]["model"] = random.choice(model_choice)
            maximal_model_size = self.M_dict[subtask]
            model_size = self.get_model_size(self.pattern, individual[subtask]["model"])
            unit_samples = 4 * (maximal_model_size / model_size) - 3
            individual[subtask]["samples"] = random.choice([1] + list(range(10, int(unit_samples), 5)))
            if self.check_budget(individual):
                candidates.append(individual)
            if len(candidates) >= batch_size:
                break
        return candidates


    def evolve(self, evaluated_population, population_size, crossover_rate, mutation_rate):
        """生成新一代种群"""
        new_population = []

        # 保留精英
        elites = sorted(evaluated_population, key=lambda x: x['score'][self.archive.environment.main_metric], reverse=True)[:population_size]  # elite size
        new_population.extend([e['params'] for e in elites])

        # 生成后代
        while len(new_population) < population_size*2:
            parent1 = self._rank_selection(evaluated_population)
            parent2 = self._rank_selection(evaluated_population)

            if random.random() < crossover_rate:
                child = self._crossover(parent1, parent2)
            else:
                child = parent1.copy()

            child = self._mutate(child, mutation_rate)

            # 预算检查
            if self.calculate_budget(child) <= self.budget:
                new_population.append(child)

        population = new_population[-population_size:]
        return population

    def _rank_selection(self, evaluated_population):
        """基于排名的选择"""
        sorted_pop = sorted(evaluated_population, key=lambda x: x['score'][self.archive.environment.main_metric], reverse=True)
        ranks = np.arange(1, len(sorted_pop) + 1)
        probs = ranks / ranks.sum()
        return sorted_pop[np.random.choice(len(sorted_pop), p=probs)]['params']

    def _crossover(self, parent1, parent2):
        """单点交叉"""
        child = {}
        subtasks = self.sub_tasks
        crossover_point = random.randint(1, len(subtasks) - 1)

        for i, subtask in enumerate(subtasks):
            if i < crossover_point:
                child[subtask] = parent1[subtask].copy()
            else:
                child[subtask] = parent2[subtask].copy()
        return child

    def _mutate(self, individual, mutation_rate):
        """随机变异"""
        mutated = individual.copy()
        for subtask, model_choice in zip(self.sub_tasks, self.model_choices):
            if random.random() < mutation_rate:  # mutation rate
                # 变异模型选择
                mutated[subtask]["model"] = random.choice(model_choice)
            if random.random() < mutation_rate:
                # 变异采样次数（±10步长）
                current = mutated[subtask]["samples"]
                mutated[subtask]["samples"] = max(1, current + random.choice([-10, 0, 10]))
        return mutated



    def gen_new_experience(self, recent_added_samples:int, init=False):
        if init:
            history_str = "\n".join(
                [f"Parameters: {h['params']}, Score: {h['score']}, Used budget: {h['cost']:.2f}, total budget: {self.budget}" for h in self.archive.history[-recent_added_samples:]]
            )
            system_prompt = "You are an expert in parameter optimization."
            prompt = f"""I am tuning test-time parameters for a complex task that can be broken down into multiple simple subtasks. The parameters include the model size and the number of samples used for each subtask. Generally, increasing the number of samples improves performance. The goal is to find the best parameter settings that maximize performance within a fixed budget.

            I will provide the task name, description, budget, and initial search history. Please help me summarize insights from the search history in the following aspect:
            
            1. The search history includes initial trials that use different model sizes under the same unit budget. In these trials, smaller models are allowed more samples, while larger models have fewer samples due to their higher cost. Please help me identify which model size we should explore further in each subtask. If the large model performs significantly better than the small model, we will choose the large one. Otherwise, if its performance is similar or only slightly better, we will prefer the smaller model.
            
            ###Task name: {self.task_name}  
            ###Task description: {self.task_desc}  
            ###Subtasks: {self.sub_tasks}  
            ###Budget: {self.budget}  
            ###Metrics: {self.archive.environment.metrics}, Main metric: {self.archive.environment.main_metric}  
            
            ###Search history: {history_str}  
            
            Please generate your response in most concise wording.
            
            ###Response:
            """
        else:
            if not no_insight2:
                insight2 = "As the number of samples increases, performance tends to improve and then level off. Please identify the search direction for finding the optimal number of samples for each subtask, especially the point after which more samples no longer improve results."
            else:
                insight2 = "None"
            if not no_insight3:
                # best for 850
                # "The total compute budget is shared across all subtasks and they are interdependent. If we could not assign the optimal model and sample count to every subtask, consider the following strategy. Prioritize each subtask and try different combinations of the budget allocation in other tasks and Balance the allocations among subtasks. "
                # insight3 = "The budget allocated to an earlier subtask influences both model selection and the optimal number of samples for subsequent subtasks. Because the total compute budget is shared across all subtasks and they are interdependent, it is often not possible to assign the optimal model and sample count to every subtask. In such cases, first allocate the optimal budget to each subtask and try to find the optimal budget allocation on others for maximal final performance. Meanwhile, . This process helps reveal which subtasks have the greatest impact on overall performance and should be prioritized in budget allocation."
                # insight3 = "Because the total budget is shared among all subtasks, how should it be allocated? Based on the history, which subtask(s) should be prioritized in budget allocation? Please reason step by step to plan a budget allocation strategy that maximizes overall task performance."
                # v2
                # insight3 = "The total compute budget is shared across all subtasks, which are interdependent. When it is not possible to assign the optimal model and sample count to every subtask, plan the search to identify the best trade-off in budget allocation across subtasks."
                # v3
                # insight3 = """The total compute budget is shared across all subtasks, which are interdependent. When it is not possible to assign the optimal model and sample count to every subtask, plan the search to find the best trade-off in budget allocation across subtasks. Use the following strategy:
                # 1. **Identify Critical Subtasks**: Rank subtasks by their impact on overall performance. Prioritize allocating more budget to higher-impact subtasks.
                # 2. **Progressive Allocation**: Starting from the first subtask, iteratively assign budget while considering the remaining budget and downstream impact. Adjust early allocations if they overly constrain later ones.
                # 3. **Simulate Alternatives**: Explore multiple plausible allocations by varying model/sample choices within budget constraints. Compare projected overall task performance across these configurations.
                # 4. **Select Best Trade-off**: Choose the configuration that maximizes overall task performance while respecting the total budget. Clearly explain trade-offs made and why certain subtasks received more or fewer resources.
                # Reason step by step and justify your decisions throughout the planning process.
                # """
                # v4
                insight3 = """The total compute budget is shared across all interdependent subtasks. When it is not possible to assign the optimal model and sample count to every subtask, plan the search to find the best trade-off in budget allocation.

Use the following strategy:

Fix One, Adjust Others: Start by selecting a strong configuration (model and sample count) for the most critical subtask based on prior knowledge or impact.

Greedy Adjustment: For the remaining subtasks, allocate budget greedily by choosing the best affordable configuration under the remaining budget.

Backtrack if Needed: If later subtasks suffer from too little budget, backtrack to revise earlier allocations and rebalance the trade-off.

Evaluate Alternatives: Compare 2–3 allocation plans that differ in which subtask is prioritized first. Choose the one that yields the best overall performance estimate.

Explain each step and the reasoning behind your decisions."""
            else:
                insight3 = "None"
            old_history_str = "\n".join(
                [f"Parameters: {h['params']}, Score: {h['score']}, Used budget: {h['cost']:.2f}, total budget: {self.budget}" for h in self.archive.history[:-recent_added_samples]]
            )
            recent_history_str = "\n".join([f"Parameters: {h['params']}, Score: {h['score']}, Used budget: {h['cost']:.2f}, total budget: {self.budget}" for h in self.archive.history[-recent_added_samples:]])
            system_prompt = "You are an expert in parameter optimization."
            prompt = f"""I am tuning test-time parameters for a complex task that can be broken down into multiple simple subtasks. The parameters include the model size and the number of samples used for each subtask. Generally, increasing the number of samples improves performance. The goal is to find the best parameter settings that maximize performance within a fixed budget.

            I will provide the task name, description, budget, and both old and recent search history.
            
            Please help me summarize insights from the recent search history in the following three aspects:
            
            1. {insight2}
            
            2. {insight3}
            
            3. If you observe that the performance variance across the search history is small, please suggest that future searches focus more on exploration strategies such as crossover, mutation, and random search, rather than strictly following the existing search patterns.
            
            ###Task name: {self.task_name}  
            ###Task description: {self.task_desc}  
            ###Subtasks: {self.sub_tasks}  
            ###Budget: {self.budget}  
            ###Metrics: {self.archive.environment.metrics}, Main metric: {self.archive.environment.main_metric}  
            
            ###Old search history: {old_history_str}  
            ###Recent search history: {recent_history_str}  
            
            Please generate your response in most concise wording, item by item.
            
            ###Response:
            """
        print("\nLog, prompt for experience:", prompt)
        experience = self.llm.generate(prompt, system_prompt)
        print("\nLog, experience:", experience)

        return experience

    def convert_budget_to_num_samples(self, budget, current_model_size, Np, Nd):
        M_large = current_model_size
        M_small = 3e9
        alpha = M_large / M_small
        beta1 = Np / Nd
        beta2 = 128 / 64
        beta3 = Np / 128
        S = beta1 * (((budget / beta2) + 1) / (alpha * beta3) - 1)
        return S

    def convert_num_samples_to_budget(self, S, M_large, Np1, Nd1, M_small=3e9, Np2=128, Nd2=64):

        alpha = M_large / M_small
        beta1 = Np1 / Nd1
        beta2 = Np2 / Nd2  # normalize to same decode length as unit config
        beta3 = Np1 / Np2

        budget = beta2 * ((alpha * beta3 / beta1) * (beta1 + S) - 1)

        return budget

    def init_candidates(self):
        candidates = []
        # budget_for_subtask = self.budget / len(self.sub_tasks)
        if self.task_name in ["2wikihopqa","2wikihopqa_75","2wikihopqa_100", "hotpotqa"]:
            budget_for_subtasks = [self.budget*0.85, self.budget*0.15]
        elif self.task_name in ["cwq", "webqsp"]:
            budget_for_subtasks = [self.budget*0.78, self.budget*0.22]
        elif self.task_name in ["taskbench_dailylifeapis"]:
            budget_for_subtasks = [self.budget*0.2, self.budget*0.24, self.budget*0.56]
        elif self.task_name in ["chatdev"]:
            budget_for_subtasks = [self.budget*0.42, self.budget*0.32, self.budget*0.26]

        if not no_insight1:
            for sub_task, budget, model_choice, Np, Nd in zip(self.sub_tasks, budget_for_subtasks, self.model_choices, Np_list, Nd_list):
                max_model_size = self.M_dict[sub_task]
                # 添加每个model size对应预算为1的采样数量, 包括大模型和小模型
                for model in model_choice:
                    candidate1 = {}
                    current_model_size = self.get_model_size(self.pattern, model)  # model_choice[0] is the minimal
                    # denominator = 4 * (max_model_size / current_model_size) - 3
                    candidate1[sub_task] = {}
                    candidate1[sub_task]['model'] = model
                    S = self.convert_budget_to_num_samples(budget, current_model_size, Np, Nd)
                    S = 1 if S < 1 else S
                    S = 60 if S > 60 else S
                    # budget = S / (2 * beta_1) + (alpha * beta_3 - 1) / 2
                    candidate1[sub_task]['budget'] = budget
                    candidate1[sub_task]['samples'] = int(S)
                    for sub_task2, budget2, model_choice2, Np2, Nd2 in zip(self.sub_tasks, budget_for_subtasks, self.model_choices, Np_list, Nd_list):
                        if sub_task2 != sub_task:
                            candidate1[sub_task2] = {}
                            # 首先尝试最大尺寸
                            current_model_size2 = self.get_model_size(self.pattern, model_choice2[-1])
                            budget_unit = self.convert_num_samples_to_budget(1, current_model_size2, Np2, Nd2)
                            print("log, budget_unit:", budget_unit)
                            if budget2 > budget_unit:
                                candidate1[sub_task2]['model'] = model_choice2[-1]
                                candidate1[sub_task2]['samples'] = 1
                                candidate1[sub_task2]['budget'] = budget2
                            else:
                                candidate1[sub_task2]['model'] = model_choice2[-2]
                                candidate1[sub_task2]['samples'] = 1

                    candidates.append(candidate1)
        else:
            candidates = self.random(1)
        return candidates



if __name__ == '__main__':
    # nohup python AgentTTS.py --task_name 2wikihopqa_75 --budget 1000 --iterations 5 --batch_size 10 > log_ours_2wikihopqa_75_budget1000.log 2>&1 &
    # nohup python AgentTTS.py --task_name 2wikihopqa_100 --budget 1000 --iterations 5 --batch_size 10 > log_ours_2wikihopqa_100_budget1000.log 2>&1 &
    # nohup python AgentTTS.py --task_name 2wikihopqa --budget 1000 > log_ours_2wikihopqa_budget1000.log 2>&1 &
    # nohup python AgentTTS.py --task_name 2wikihopqa --budget 500 --iterations 5 --batch_size 10 > log_ours_2wikihopqa_budget500.log 2>&1 &
    # nohup python AgentTTS.py --task_name 2wikihopqa --budget 850 --iterations 5 --batch_size 10 > log_ours_2wikihopqa_budget850.log 2>&1 &
    # nohup python AgentTTS.py --task_name 2wikihopqa --budget 2000 --iterations 5 --batch_size 10 > log_ours_2wikihopqa_budget2000.log 2>&1 &
    # nohup python AgentTTS.py --task_name cwq --budget 550 > log_ours_cwq_budget550.log 2>&1 &
    # nohup python AgentTTS.py --task_name webqsp --budget 550 > log_ours_webqsp_budget550.log 2>&1 &
    # nohup python AgentTTS.py --task_name taskbench_dailylifeapis --budget 2000 > log_ours_taskbench_dailylifeapis_budget2000.log 2>&1 &
    # nohup python AgentTTS.py --task_name chatdev --budget 2000 > log_ours_taskbench_chatdev_budget2000.log 2>&1 &

    # ablation study
    # nohup python AgentTTS.py --task_name 2wikihopqa --budget 900 --iterations 5 --batch_size 10 --no_insight1 > log_ours_2wikihopqa_budget900_noinsight1.log 2>&1 &
    # nohup python AgentTTS.py --task_name 2wikihopqa --budget 900 --iterations 5 --batch_size 10 --no_insight2 > log_ours_2wikihopqa_budget900_noinsight2.log 2>&1 &
    # nohup python AgentTTS.py --task_name 2wikihopqa --budget 900 --iterations 5 --batch_size 10 --no_insight3 > log_ours_2wikihopqa_budget900_noinsight3.log 2>&1 &
    # nohup python AgentTTS.py --task_name 2wikihopqa --budget 850 --iterations 5 --batch_size 10 --no_insight1 > log_ours_2wikihopqa_budget850_noinsight1.log 2>&1 &
    # nohup python AgentTTS.py --task_name 2wikihopqa --budget 850 --iterations 5 --batch_size 10 --no_insight2 > log_ours_2wikihopqa_budget850_noinsight2.log 2>&1 &
    # nohup python AgentTTS.py --task_name 2wikihopqa --budget 850 --iterations 5 --batch_size 10 --no_insight3 > log_ours_2wikihopqa_budget850_noinsight3.log 2>&1 &

    parser = argparse.ArgumentParser()
    parser.add_argument('--task_name', type=str, default="2wikihopqa")
    parser.add_argument('--budget', type=int, default=1000,)
    parser.add_argument('--iterations', type=int, default=5,)
    parser.add_argument('--batch_size', type=int, default=10,)
    parser.add_argument('--no_insight1', action='store_true')
    parser.add_argument('--no_insight2', action='store_true')
    parser.add_argument('--no_insight3', action='store_true')

    args = parser.parse_args()
    print(args)

    unit_budgets = {
        "2wikihopqa": {"Retrieval": 814, "Question Answering": 115},
        "2wikihopqa_75": {"Retrieval": 814, "Question Answering": 115},
        "2wikihopqa_100": {"Retrieval": 814, "Question Answering": 115},
        "hotpotqa": {},
        "cwq": {},
        "webqsp": {},
        "taskbench_dailylifeapis": {"Task Decomposition": 395, "Tool Selection": 465, "Parameter Prediction": 1118},
        "chatdev": {"Coding": 745, "Static Testing": 558, "Dynamic Testing": 465},
    }
    # for ablation study
    no_insight1 = args.no_insight1
    no_insight2 = args.no_insight2
    no_insight3 = args.no_insight3
    print("\nNo insights (1, 2, 3):", no_insight1, no_insight2, no_insight3)
    name = args.task_name
    budget = args.budget
    if "2wikihopqa" in name:
        desc = "This task Retreval-then-generate has two subtasks. The first subtask is retrieval: given a question and a set of 100 text chunks, the model needs to select the most relevant chunks that can help answer the question. The second subtask is question answering: based on the retrieved chunks, the model should generate a response that answers the question accurately."
        model_choices = [["Qwen/Qwen2.5-7B-Instruct-AWQ", "Qwen/Qwen2.5-32B-Instruct-AWQ", "Qwen/Qwen2.5-72B-Instruct-AWQ"], ["meta-llama/Llama-3.2-3B-Instruct", "meta-llama/Llama-3.1-8B-Instruct", "meta-llama/Llama-3.1-70B-Instruct"]]
        pattern = r"-(\d+)B-"
        sub_tasks = ["Retrieval", "Question Answering"]
        Np_list = [2048, 256]
        Nd_list = [128, 64]
        model_space = {
            sub_task: {'model_choice': model_choice, "prompt_length": Np, "generation_length": Nd} for sub_task, model_choice, Np, Nd in zip(sub_tasks, model_choices, Np_list, Nd_list)
        }
    elif name == "hotpotqa":
        desc = "This task Retreval-then-generate has two subtasks. The first subtask is retrieval: given a question and a set of 100 text chunks, the model needs to select the most relevant chunks that can help answer the question. The second subtask is question answering: based on the retrieved chunks, the model should generate a response that answers the question accurately."
        model_choices = [["Qwen/Qwen2.5-7B-Instruct-AWQ", "Qwen/Qwen2.5-32B-Instruct-AWQ", "Qwen/Qwen2.5-72B-Instruct-AWQ"], ["meta-llama/Llama-3.2-3B-Instruct", "meta-llama/Llama-3.1-8B-Instruct", "meta-llama/Llama-3.1-70B-Instruct"]]
        pattern = r"-(\d+)B-"
        sub_tasks = ["Retrieval", "Question Answering"]
        Np_list = [2048, 256]
        Nd_list = [128, 64]
        model_space = {
            sub_task: {'model_choice': model_choice, "prompt_length": Np, "generation_length": Nd} for sub_task, model_choice, Np, Nd in zip(sub_tasks, model_choices, Np_list, Nd_list)
        }
    elif name in ["cwq", "webqsp"]:
        desc = "This task Retreval-then-generate has two subtasks. The first subtask is retrieval: given a question and a set of 100 knowledge triplets, the model needs to select the most relevant triplets that can help answer the question. The second subtask is question answering: based on the retrieved knowledge triplets, the model should generate a response that answers the question accurately."
        model_choices = [["Qwen/Qwen2.5-7B-Instruct-AWQ", "Qwen/Qwen2.5-72B-Instruct-AWQ"], ["meta-llama/Llama-3.1-8B-Instruct", "meta-llama/Llama-3.1-70B-Instruct"]]
        pattern = r"-(\d+)B-"
        sub_tasks = ["Retrieval", "Question Answering"]
        Np_list = [1024, 256]
        Nd_list = [64, 64]
        model_space = {
            sub_task: {'model_choice': model_choice, "prompt_length": Np, "generation_length": Nd} for sub_task, model_choice, Np, Nd in zip(sub_tasks, model_choices, Np_list, Nd_list)
        }
    elif name in ["taskbench_dailylifeapis"]:
        desc = "This task Taskbench has three subtasks: task decomposition, tool selection, and parameter prediction. The first subtask is task decomposition: upon receiving a user request, the large language model performs task decomposition, generating a sequence of task steps. The second subtask is tool selection: for each task step, the model should select appropriate tools. The third subtask is parameter prediction: for each selected tool, the model should predict its parameters."
        model_choices = [["meta-llama/Llama-3.1-8B-Instruct", "meta-llama/Llama-3.1-70B-Instruct"], ["meta-llama/Llama-3.1-8B-Instruct", "meta-llama/Llama-3.1-70B-Instruct"], ["meta-llama/Llama-3.1-8B-Instruct", "meta-llama/Llama-3.1-70B-Instruct"]]
        pattern = r"-(\d+)B-"
        sub_tasks = ["Task Decomposition", "Tool Selection", "Parameter Prediction"]
        Np_list = [1024, 1024, 1024]
        Nd_list = [64, 256, 2048]
        model_space = {
            sub_task: {'model_choice': model_choice, "prompt_length": Np, "generation_length": Nd} for sub_task, model_choice, Np, Nd in zip(sub_tasks, model_choices, Np_list, Nd_list)
        }
    elif name in ["chatdev"]:
        desc = "This task Chatdev is an automated software development task. It has three subtasks: coding, static testing, and dynamic testing."
        model_choices = [["llama3b", "llama70b"],["llama3b", "llama70b"],["llama3b", "llama70b"],]
        pattern = r"-(\d+)B-"
        sub_tasks = ["Coding", "Static Testing", "Dynamic Testing"]
        Np_list = [1024, 1024, 1024]
        Nd_list = [1024, 512, 256]
        model_space = {
            sub_task: {'model_choice': model_choice, "prompt_length": Np, "generation_length": Nd} for sub_task, model_choice, Np, Nd in zip(sub_tasks, model_choices, Np_list, Nd_list)
        }


    # init an agent object
    llm = ChatGPT()
    example = {}
    for sub_task, model_choice in zip(sub_tasks, model_choices):
        example[sub_task] = {}
        example[sub_task]['model'] = model_choice[-1]
        example[sub_task]['samples'] = 1
        example[sub_task]['budget'] = unit_budgets[name][sub_task]
    example = {"candidates": [
        example,
    ]}
    agent = RAGAgentTTS(name, desc, budget, model_choices, pattern, sub_tasks, Np_list, Nd_list, llm, example)
    # init a task object
    hub = Task(name, desc, budget, model_choices, pattern, sub_tasks, agent)
    results, trace = hub.get_tts_strategy(iterations=args.iterations, batch_size=args.batch_size)
    print("Trace:", trace)

